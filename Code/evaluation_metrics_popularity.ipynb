{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec44b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import requests_cache\n",
    "import urllib.parse\n",
    "import sqlite3\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Enable caching of HTTP requests\n",
    "requests_cache.install_cache('wikidata_cache', backend='memory', expire_after=86400)\n",
    "\n",
    "def remove_disambiguator(label):\n",
    "    # Remove trailing ' (number)' from label, e.g., 'father (1)' -> 'father'\n",
    "    return re.sub(r' \\(\\d+\\)$', '', label)\n",
    "\n",
    "def safe_query(db_file, query, retries=50, delay=2):\n",
    "    \"\"\"Attempts to execute a query on an SQLite database with retries.\"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            conn = sqlite3.connect(f'file:{db_file}?mode=ro', uri=True)\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(query)\n",
    "            result = cursor.fetchall()\n",
    "            rows = list(set(remove_disambiguator(row[0]) for row in result))\n",
    "            print(f\"Total entities found in database: {len(rows)}\")\n",
    "            return rows\n",
    "        except sqlite3.DatabaseError as e:\n",
    "            print(f\"[{db_file}] Attempt {attempt+1} failed: {e}\")\n",
    "            time.sleep(delay)\n",
    "        finally:\n",
    "            if 'conn' in locals():\n",
    "                conn.close()\n",
    "    print(f\"[{db_file}] Failed after {retries} attempts.\")\n",
    "    return list()\n",
    "\n",
    "def get_wikidata_id(entity_name, language=\"en\", retries=50, delay=4):\n",
    "    url = \"https://www.wikidata.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"wbsearchentities\",\n",
    "        \"format\": \"json\",\n",
    "        \"language\": language,\n",
    "        \"search\": entity_name\n",
    "    }\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=10)\n",
    "            data = response.json()\n",
    "            if data.get('search'):\n",
    "                return data['search'][0]['id']\n",
    "            return None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed for {entity_name} (attempt {attempt+1}): {e}\")\n",
    "            time.sleep(delay)\n",
    "    return None\n",
    "\n",
    "def get_statement_count(wikidata_id):\n",
    "    url = f\"https://www.wikidata.org/wiki/Special:EntityData/{wikidata_id}.json\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    try:\n",
    "        entity = data['entities'][wikidata_id]\n",
    "        return len(entity.get('claims', {}))\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "def fetch_entity_info(name, language=\"en\"):\n",
    "    wikidata_id = get_wikidata_id(name, language=language)\n",
    "    if wikidata_id:\n",
    "        count = get_statement_count(wikidata_id)\n",
    "        return (name, wikidata_id, count)\n",
    "    else:\n",
    "        return (name, None, 0)\n",
    "\n",
    "def fetch_and_sort_by_statements(entities, language=\"en\"):\n",
    "    results = []\n",
    "    not_found = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(fetch_entity_info, name, language): name for name in entities}\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Fetching statement counts from Wikidata for {len(entities)} entities\"):\n",
    "            name, wikidata_id, count = future.result()\n",
    "            if wikidata_id:\n",
    "                results.append((name, wikidata_id, count))\n",
    "            else:\n",
    "                not_found.append(name)\n",
    "    \n",
    "    # Sort by statement count, descending\n",
    "    results.sort(key=lambda x: x[2], reverse=True)\n",
    "    print(f\"\\nTotal entities also found in Wikidata: {len(results)}\")\n",
    "    print(f\"Total not found in Wikidata: {len(not_found)}\")\n",
    "    if not_found:\n",
    "        print(\"Some of the missing entities:\")\n",
    "        for nf in not_found[:10]:  # show first 10 missing entries\n",
    "            print(f\" - {nf}\")\n",
    "    \n",
    "    # Calculate top k%\n",
    "    #k = int(len(results) * (k_percent / 100))\n",
    "    #print(f\"Top {k_percent}% of entities: {k} entities\")\n",
    "    return results#[:k]\n",
    "\n",
    "def bucket_by_percentiles(results):\n",
    "    \"\"\"Split sorted entity results into 4 percentile buckets.\"\"\"\n",
    "    total = len(results)\n",
    "    buckets = {\n",
    "        \"0-25%\": results[int(total * 0.75):],\n",
    "        \"25-50%\": results[int(total * 0.5):int(total * 0.75)],\n",
    "        \"50-75%\": results[int(total * 0.25):int(total * 0.5)],\n",
    "        \"75-100%\": results[:int(total * 0.25)]\n",
    "    }\n",
    "    return buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d953904",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = \"SELECT name FROM node WHERE type=\\\"instance\\\" ;\"\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_language_code(db_file):\n",
    "    # Try to extract language code from filename, fallback to 'en' if not found\n",
    "    match = re.search(r'_([A-Z]{2})_temp', db_file)\n",
    "    if match:\n",
    "        lang = match.group(1).lower()\n",
    "        # PO is not a standard code, map to 'pl' for Polish, etc.\n",
    "        lang_map = {'po': 'pl'}\n",
    "        return lang_map.get(lang, lang)\n",
    "    return 'en'\n",
    "\n",
    "db_files = [\n",
    "    './base_setting/babylonGPTKB_termination_seed1_EN_temp0_1st.db',\n",
    "    './base_setting/babylonGPTKB_termination_seed1_EN_temp0_2nd.db',\n",
    "    './base_setting/babylonGPTKB_termination_seed1_EN_temp0_3rd.db',\n",
    "    './base_setting/babylonGPTKB_termination_seed1_EN_temp0_4th.db',\n",
    "    './base_setting/babylonGPTKB_termination_seed1_EN_temp0_5th.db',\n",
    "    './base_setting/babylonGPTKB_termination_seed1_EN_temp0_6th.db',\n",
    "    './base_setting/babylonGPTKB_termination_seed1_EN_temp0_7th.db',\n",
    "    './base_setting/babylonGPTKB_termination_seed1_EN_temp0_8th.db',\n",
    "    './base_setting/babylonGPTKB_termination_seed1_EN_temp0_9th.db',\n",
    "    './base_setting/babylonGPTKB_termination_seed1_EN_temp0_10th.db',\n",
    "\n",
    "    #'./seed_variation/babylonGPTKB_termination_seed2_EN_temp0.db',\n",
    "    #'./seed_variation/babylonGPTKB_termination_seed3_EN_temp0.db',\n",
    "    #'./seed_variation/babylonGPTKB_termination_seed4_EN_temp0.db',\n",
    "    #'./seed_variation/babylonGPTKB_termination_seed5_EN_temp0.db',\n",
    "    #'./seed_variation/babylonGPTKB_termination_seed6_EN_temp0.db',\n",
    "    #'./seed_variation/babylonGPTKB_termination_seed7_EN_temp0.db',\n",
    "    #'./seed_variation/babylonGPTKB_termination_seed8_EN_temp0.db',\n",
    "    #'./seed_variation/babylonGPTKB_termination_seed9_EN_temp0.db',\n",
    "    #'./seed_variation/babylonGPTKB_termination_seed10_EN_temp0.db',\n",
    "\n",
    "    #'./rand_variation/babylonGPTKB_termination_seed1_EN_temp1_1st.db',\n",
    "    #'./rand_variation/babylonGPTKB_termination_seed1_EN_temp1_2nd.db',\n",
    "    #'./rand_variation/babylonGPTKB_termination_seed1_EN_temp1_3rd.db',\n",
    "    #'./rand_variation/babylonGPTKB_termination_seed1_EN_temp1_4th.db',\n",
    "    #'./rand_variation/babylonGPTKB_termination_seed1_EN_temp1_5th.db',\n",
    "    #'./rand_variation/babylonGPTKB_termination_seed1_EN_temp1_6th.db',\n",
    "    #'./rand_variation/babylonGPTKB_termination_seed1_EN_temp1_7th.db',\n",
    "    #'./rand_variation/babylonGPTKB_termination_seed1_EN_temp1_8th.db',\n",
    "    #'./rand_variation/babylonGPTKB_termination_seed1_EN_temp1_9th.db',\n",
    "    #'./rand_variation/babylonGPTKB_termination_seed1_EN_temp1_10th.db',\n",
    "\n",
    "    #'./lang_variation/backtranslation_EN/babylonGPTKB_DE-EN_20250630_100431.db',\n",
    "    #'./lang_variation/backtranslation_EN/babylonGPTKB_ES-EN_20250616_091934.db',\n",
    "    #'./lang_variation/backtranslation_EN/babylonGPTKB_FR-EN_20250627_151851.db',\n",
    "    #'./lang_variation/backtranslation_EN/babylonGPTKB_IT-EN_20250628_132420.db',\n",
    "    #'./lang_variation/backtranslation_EN/babylonGPTKB_PO-EN_20250623_102904.db',\n",
    "    #'./lang_variation/backtranslation_EN/babylonGPTKB_PT-EN_20250623_104443.db',\n",
    "    #'./lang_variation/backtranslation_EN/babylonGPTKB_RU-EN_20250612_163509.db',\n",
    "    #'./lang_variation/backtranslation_EN/babylonGPTKB_SV-EN_20250703_095332.db',\n",
    "    #'./lang_variation/backtranslation_EN/babylonGPTKB_TR-EN_20250620_104047.db',\n",
    "\n",
    "    #'./lang_variation/ID_babylonGPTKB_seed1_DE_temp0.db',\n",
    "    #'./lang_variation/ID_babylonGPTKB_seed1_ES_temp0.db',\n",
    "    #'./lang_variation/ID_babylonGPTKB_seed1_FR_temp0.db',\n",
    "    #'./lang_variation/ID_babylonGPTKB_seed1_IT_temp0.db',\n",
    "    #'./lang_variation/ID_babylonGPTKB_seed1_PO_temp0.db',\n",
    "    #'./lang_variation/ID_babylonGPTKB_seed1_PT_temp0.db',\n",
    "    #'./lang_variation/ID_babylonGPTKB_seed1_RU_temp0.db',\n",
    "    #'./lang_variation/ID_babylonGPTKB_seed1_SV_temp0.db',\n",
    "    #'./lang_variation/ID_babylonGPTKB_seed1_TR_temp0.db',\n",
    "\n",
    "    #'./topic_variation/TBBT/TBBT_GPTKB_seed1_EN_temp0_1.db',\n",
    "    #'./topic_variation/TBBT/TBBT_GPTKB_seed1_EN_temp0_2.db',\n",
    "    #'./topic_variation/TBBT/TBBT_GPTKB_seed1_EN_temp0_3.db',\n",
    "    #'./topic_variation/TBBT/TBBT_GPTKB_seed1_EN_temp0_4.db',\n",
    "    #'./topic_variation/TBBT/TBBT_GPTKB_seed1_EN_temp0_5.db',\n",
    "    #'./topic_variation/TBBT/TBBT_GPTKB_seed1_EN_temp0_6.db',\n",
    "    #'./topic_variation/TBBT/TBBT_GPTKB_seed1_EN_temp0_7.db',\n",
    "    #'./topic_variation/TBBT/TBBT_GPTKB_seed1_EN_temp0_8.db',\n",
    "    #'./topic_variation/TBBT/TBBT_GPTKB_seed1_EN_temp0_9.db',\n",
    "    #'./topic_variation/TBBT/TBBT_GPTKB_seed1_EN_temp0_10.db',\n",
    "\n",
    "    #'./topic_variation/dax40/dax40GPTKB_seed1_EN_temp0_1.db',\n",
    "    #'./topic_variation/dax40/dax40GPTKB_seed1_EN_temp0_2.db',\n",
    "    #'./topic_variation/dax40/dax40GPTKB_seed1_EN_temp0_3.db',\n",
    "    #'./topic_variation/dax40/dax40GPTKB_seed1_EN_temp0_4.db',\n",
    "    #'./topic_variation/dax40/dax40GPTKB_seed1_EN_temp0_5.db',\n",
    "    #'./topic_variation/dax40/dax40GPTKB_seed1_EN_temp0_6.db',\n",
    "    #'./topic_variation/dax40/dax40GPTKB_seed1_EN_temp0_7.db',\n",
    "    #'./topic_variation/dax40/dax40GPTKB_seed1_EN_temp0_8.db',\n",
    "    #'./topic_variation/dax40/dax40GPTKB_seed1_EN_temp0_9.db',\n",
    "    #'./topic_variation/dax40/dax40GPTKB_seed1_EN_temp0_10.db',\n",
    "    ]\n",
    "\n",
    "all_0_25_labels = []\n",
    "all_25_50_labels = []\n",
    "all_50_75_labels = []\n",
    "all_75_100_labels = []\n",
    "all_not_found_labels = []\n",
    "\n",
    "for db_file in db_files:\n",
    "    language = 'en' #extract_language_code(db_file)\n",
    "    entity_list = safe_query(db_file, sql_query)\n",
    "    top_entities = fetch_and_sort_by_statements(entity_list, language=language)\n",
    "\n",
    "    def fetch_and_sort_by_statements_with_not_found(entities, language):\n",
    "        results = []\n",
    "        not_found = []\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures = {executor.submit(fetch_entity_info, name, language): name for name in entities}\n",
    "            for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Fetching statement counts from Wikidata for {len(entities)} entities\"):\n",
    "                name, wikidata_id, count = future.result()\n",
    "                if wikidata_id:\n",
    "                    results.append((name, wikidata_id, count))\n",
    "                else:\n",
    "                    not_found.append(name)\n",
    "\n",
    "        results.sort(key=lambda x: x[2], reverse=True)\n",
    "        return results, not_found\n",
    "\n",
    "    top_entities, not_found_entities = fetch_and_sort_by_statements_with_not_found(entity_list, language)\n",
    "\n",
    "    buckets = bucket_by_percentiles(top_entities)\n",
    "\n",
    "    bucket_0_25 = buckets[\"0-25%\"]\n",
    "    bucket_25_50 = buckets[\"25-50%\"]\n",
    "    bucket_50_75 = buckets[\"50-75%\"]\n",
    "    bucket_75_100 = buckets[\"75-100%\"]\n",
    "\n",
    "    # Print summaries\n",
    "    for label, entities in buckets.items():\n",
    "        print(f\"\\nBucket {label}: {len(entities)} entities.\")\n",
    "        for name, qid, count in entities[:5]:  # Show a few from each\n",
    "            print(f\" - {name} ({qid}) - {count} statements\")\n",
    "\n",
    "    all_0_25_labels.append([i[0] for i in bucket_0_25])\n",
    "    all_25_50_labels.append([i[0] for i in bucket_25_50])\n",
    "    all_50_75_labels.append([i[0] for i in bucket_50_75])\n",
    "    all_75_100_labels.append([i[0] for i in bucket_75_100])\n",
    "    all_not_found_labels.append(not_found_entities)\n",
    "\n",
    "(set_1_0_25, set_2_0_25, set_3_0_25, set_4_0_25, set_5_0_25,\n",
    " set_6_0_25, set_7_0_25, set_8_0_25, set_9_0_25, set_10_0_25\n",
    " ) = all_0_25_labels\n",
    "\n",
    "(set_1_25_50, set_2_25_50, set_3_25_50, set_4_25_50, set_5_25_50,\n",
    " set_6_25_50, set_7_25_50, set_8_25_50, set_9_25_50, set_10_25_50\n",
    " ) = all_25_50_labels\n",
    "\n",
    "(set_1_50_75, set_2_50_75, set_3_50_75, set_4_50_75, set_5_50_75,\n",
    " set_6_50_75, set_7_50_75, set_8_50_75, set_9_50_75, set_10_50_75\n",
    " ) = all_50_75_labels\n",
    "\n",
    "(set_1_75_100, set_2_75_100, set_3_75_100, set_4_75_100, set_5_75_100,\n",
    " set_6_75_100, set_7_75_100, set_8_75_100, set_9_75_100, set_10_75_100\n",
    " ) = all_75_100_labels\n",
    "\n",
    "(set_1_not_found, set_2_not_found, set_3_not_found, set_4_not_found, set_5_not_found,\n",
    " set_6_not_found, set_7_not_found, set_8_not_found, set_9_not_found, set_10_not_found\n",
    " ) = all_not_found_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f124645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVERAGE JACCARD SIMILARITY\n",
    "\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "\n",
    "def jaccard_similarity(list_a, list_b):\n",
    "    set_a, set_b = set(list_a), set(list_b)\n",
    "    return len(set_a & set_b) / len(set_a | set_b)\n",
    "\n",
    "### 0-25 ###\n",
    "sets = [set_1_0_25, set_2_0_25, set_3_0_25, set_4_0_25, set_5_0_25,\n",
    " set_6_0_25, set_7_0_25, set_8_0_25, set_9_0_25, set_10_0_25]\n",
    "set_names = ['set_1_0_25', 'set_2_0_25', 'set_3_0_25', 'set_4_0_25', 'set_5_0_25',\n",
    " 'set_6_0_25', 'set_7_0_25', 'set_8_0_25', 'set_9_0_25', 'set_10_0_25']\n",
    "total_similarity = 0\n",
    "pair_count = 0\n",
    "for (i, set_a), (j, set_b) in combinations(enumerate(sets), 2):\n",
    "    sim = jaccard_similarity(set_a, set_b)\n",
    "    print(f\"Jaccard({set_names[i]} ∩ {set_names[j]}) = {sim:.4f}\")\n",
    "    total_similarity += sim\n",
    "    pair_count += 1\n",
    "average_similarity = total_similarity / pair_count\n",
    "std_similarity = np.std([jaccard_similarity(set_a, set_b) for (i, set_a), (j, set_b) in combinations(enumerate(sets), 2)])\n",
    "print(f\"\\nAverage pairwise Jaccard similarity BUCKET 0-25%: {average_similarity:.4f} ± {std_similarity:.4f}\")\n",
    "\n",
    "### 25-50 ###\n",
    "sets = [set_1_25_50, set_2_25_50, set_3_25_50, set_4_25_50, set_5_25_50,\n",
    " set_6_25_50, set_7_25_50, set_8_25_50, set_9_25_50, set_10_25_50]\n",
    "set_names = ['set_1_25_50', 'set_2_25_50', 'set_3_25_50', 'set_4_25_50', 'set_5_25_50',\n",
    " 'set_6_25_50', 'set_7_25_50', 'set_8_25_50', 'set_9_25_50', 'set_10_25_50']\n",
    "total_similarity = 0\n",
    "pair_count = 0\n",
    "for (i, set_a), (j, set_b) in combinations(enumerate(sets), 2):\n",
    "    sim = jaccard_similarity(set_a, set_b)\n",
    "    print(f\"Jaccard({set_names[i]} ∩ {set_names[j]}) = {sim:.4f}\")\n",
    "    total_similarity += sim\n",
    "    pair_count += 1\n",
    "average_similarity = total_similarity / pair_count\n",
    "std_similarity = np.std([jaccard_similarity(set_a, set_b) for (i, set_a), (j, set_b) in combinations(enumerate(sets), 2)])\n",
    "print(f\"\\nAverage pairwise Jaccard similarity BUCKET 25-50%: {average_similarity:.4f} ± {std_similarity:.4f}\")\n",
    "\n",
    "### 50-75 ###\n",
    "sets = [set_1_50_75, set_2_50_75, set_3_50_75, set_4_50_75, set_5_50_75,\n",
    " set_6_50_75, set_7_50_75, set_8_50_75, set_9_50_75, set_10_50_75]\n",
    "set_names = ['set_1_50_75', 'set_2_50_75', 'set_3_50_75', 'set_4_50_75', 'set_5_50_75',\n",
    " 'set_6_50_75', 'set_7_50_75', 'set_8_50_75', 'set_9_50_75', 'set_10_50_75']\n",
    "total_similarity = 0\n",
    "pair_count = 0\n",
    "for (i, set_a), (j, set_b) in combinations(enumerate(sets), 2):\n",
    "    sim = jaccard_similarity(set_a, set_b)\n",
    "    print(f\"Jaccard({set_names[i]} ∩ {set_names[j]}) = {sim:.4f}\")\n",
    "    total_similarity += sim\n",
    "    pair_count += 1\n",
    "average_similarity = total_similarity / pair_count\n",
    "std_similarity = np.std([jaccard_similarity(set_a, set_b) for (i, set_a), (j, set_b) in combinations(enumerate(sets), 2)])\n",
    "print(f\"\\nAverage pairwise Jaccard similarity BUCKET 50-75%: {average_similarity:.4f} ± {std_similarity:.4f}\")\n",
    "\n",
    "### 75-100 ###\n",
    "sets = [set_1_75_100, set_2_75_100, set_3_75_100, set_4_75_100, set_5_75_100,\n",
    " set_6_75_100, set_7_75_100, set_8_75_100, set_9_75_100, set_10_75_100]\n",
    "set_names = ['set_1_75_100', 'set_2_75_100', 'set_3_75_100', 'set_4_75_100', 'set_5_75_100',\n",
    " 'set_6_75_100', 'set_7_75_100', 'set_8_75_100', 'set_9_75_100', 'set_10_75_100']\n",
    "total_similarity = 0\n",
    "pair_count = 0\n",
    "for (i, set_a), (j, set_b) in combinations(enumerate(sets), 2):\n",
    "    sim = jaccard_similarity(set_a, set_b)\n",
    "    print(f\"Jaccard({set_names[i]} ∩ {set_names[j]}) = {sim:.4f}\")\n",
    "    total_similarity += sim\n",
    "    pair_count += 1\n",
    "average_similarity = total_similarity / pair_count\n",
    "std_similarity = np.std([jaccard_similarity(set_a, set_b) for (i, set_a), (j, set_b) in combinations(enumerate(sets), 2)])\n",
    "print(f\"\\nAverage pairwise Jaccard similarity BUCKET 75-100%: {average_similarity:.4f} ± {std_similarity:.4f}\")\n",
    "\n",
    "### NOT FOUND ###\n",
    "sets = [set_1_not_found, set_2_not_found, set_3_not_found, set_4_not_found, set_5_not_found,\n",
    " set_6_not_found, set_7_not_found, set_8_not_found, set_9_not_found, set_10_not_found]\n",
    "set_names = ['set_1_not_found', 'set_2_not_found', 'set_3_not_found', 'set_4_not_found', 'set_5_not_found',\n",
    " 'set_6_not_found', 'set_7_not_found', 'set_8_not_found', 'set_9_not_found', 'set_10_not_found']\n",
    "total_similarity = 0\n",
    "pair_count = 0\n",
    "for (i, set_a), (j, set_b) in combinations(enumerate(sets), 2):\n",
    "    sim = jaccard_similarity(set_a, set_b)\n",
    "    print(f\"Jaccard({set_names[i]} ∩ {set_names[j]}) = {sim:.4f}\")\n",
    "    total_similarity += sim\n",
    "    pair_count += 1\n",
    "average_similarity = total_similarity / pair_count\n",
    "std_similarity = np.std([jaccard_similarity(set_a, set_b) for (i, set_a), (j, set_b) in combinations(enumerate(sets), 2)])\n",
    "print(f\"\\nAverage pairwise Jaccard similarity NOT FOUND: {average_similarity:.4f} ± {std_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66f6963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COSINE-BASED HAUSDORFF SIMILARITY: BUCKETS vs FULL SETS OF OTHER RUNS\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')\n",
    "#model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2', device='cuda')\n",
    "\n",
    "def average_cosine_hausdorff_similarity_buckets(set1, set2, match_threshold=0.95):\n",
    "    if not set1 or not set2:\n",
    "        return 0, 0, 0, 0, len(set1), len(set2)\n",
    "    embeddings1 = model.encode(list(set1), convert_to_numpy=True, batch_size=64)\n",
    "    embeddings2 = model.encode(list(set2), convert_to_numpy=True, batch_size=64)\n",
    "    distances_1 = cosine_distances(embeddings1, embeddings2)\n",
    "    avg_distance1 = np.mean(np.min(distances_1, axis=1))\n",
    "    avg_similarity = 1 - avg_distance1\n",
    "    sim_matrix_1 = 1 - distances_1\n",
    "    matches1 = np.sum(np.max(sim_matrix_1, axis=1) >= match_threshold)\n",
    "    len1 = len(set1)\n",
    "    len2 = len(set2)\n",
    "    pct1 = 100 * matches1 / len1 if len1 > 0 else 0\n",
    "    return avg_similarity, matches1, pct1, len1, len2\n",
    "\n",
    "bucket_types = ['0_25', '25_50', '50_75', '75_100', 'not_found']\n",
    "buckets = {\n",
    "    '0_25': [set_1_0_25, set_2_0_25, set_3_0_25, set_4_0_25, set_5_0_25, set_6_0_25, set_7_0_25, set_8_0_25, set_9_0_25, set_10_0_25],\n",
    "    '25_50': [set_1_25_50, set_2_25_50, set_3_25_50, set_4_25_50, set_5_25_50, set_6_25_50, set_7_25_50, set_8_25_50, set_9_25_50, set_10_25_50],\n",
    "    '50_75': [set_1_50_75, set_2_50_75, set_3_50_75, set_4_50_75, set_5_50_75, set_6_50_75, set_7_50_75, set_8_50_75, set_9_50_75, set_10_50_75],\n",
    "    '75_100': [set_1_75_100, set_2_75_100, set_3_75_100, set_4_75_100, set_5_75_100, set_6_75_100, set_7_75_100, set_8_75_100, set_9_75_100, set_10_75_100],\n",
    "    'not_found': [set_1_not_found, set_2_not_found, set_3_not_found, set_4_not_found, set_5_not_found, set_6_not_found, set_7_not_found, set_8_not_found, set_9_not_found, set_10_not_found]\n",
    "}\n",
    "full_sets = [\n",
    "    set_1_0_25 + set_1_25_50 + set_1_50_75 + set_1_75_100 + set_1_not_found,\n",
    "    set_2_0_25 + set_2_25_50 + set_2_50_75 + set_2_75_100 + set_2_not_found,\n",
    "    set_3_0_25 + set_3_25_50 + set_3_50_75 + set_3_75_100 + set_3_not_found,\n",
    "    set_4_0_25 + set_4_25_50 + set_4_50_75 + set_4_75_100 + set_4_not_found,\n",
    "    set_5_0_25 + set_5_25_50 + set_5_50_75 + set_5_75_100 + set_5_not_found,     \n",
    "    set_6_0_25 + set_6_25_50 + set_6_50_75 + set_6_75_100 + set_6_not_found,     \n",
    "    set_7_0_25 + set_7_25_50 + set_7_50_75 + set_7_75_100 + set_7_not_found,\n",
    "    set_8_0_25 + set_8_25_50 + set_8_50_75 + set_8_75_100 + set_8_not_found,\n",
    "    set_9_0_25 + set_9_25_50 + set_9_50_75 + set_9_75_100 + set_9_not_found,\n",
    "    set_10_0_25 + set_10_25_50 + set_10_50_75 + set_10_75_100 + set_10_not_found\n",
    "]\n",
    "for bucket_name in bucket_types:     \n",
    "    print(f\"\\n=== Cosine-based Hausdorff: {bucket_name.replace('_', '-')}% buckets vs full sets of other runs (bucket_i to full_j, i ≠ j, one direction only) ===\")\n",
    "    similarities = []\n",
    "    match_percentages = []\n",
    "    for i, bucket in enumerate(buckets[bucket_name]): \n",
    "        for j, full in enumerate(full_sets):\n",
    "            if i == j:\n",
    "                continue  # skip comparing to own run           \n",
    "            similarity, matches, pct, len1, len2 = average_cosine_hausdorff_similarity_buckets(bucket, full)\n",
    "            print(f\"Bucket {bucket_name} of set {i+1} vs full set {j+1}: Similarity: {similarity:.4f}, Matches {i+1}→{j+1}: {matches}/{len1} ({pct:.1f}%)\")\n",
    "            similarities.append(similarity)\n",
    "            match_percentages.append(pct)\n",
    "    if similarities:\n",
    "        print(f\"Overall Avg Cosine-based Hausdorff Similarity {bucket_name.replace('_', '-')}%: {np.mean(similarities):.4f} ± {np.std(similarities):.4f}\")\n",
    "        print(f\"Overall Avg Match Percentage {bucket_name.replace('_', '-')}%: {np.mean(match_percentages):.2f}% ± {np.std(match_percentages):.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
