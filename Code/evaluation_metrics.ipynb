{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e548e83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "\n",
    "db_files = [\n",
    "    './base_setting/babylonGPTKB_termination_seed1_EN_temp0_1st.db',\n",
    "    './base_setting/babylonGPTKB_termination_seed1_EN_temp0_2nd.db',\n",
    "    './base_setting/babylonGPTKB_termination_seed1_EN_temp0_3rd.db',\n",
    "    './base_setting/babylonGPTKB_termination_seed1_EN_temp0_4th.db',\n",
    "    './base_setting/babylonGPTKB_termination_seed1_EN_temp0_5th.db',\n",
    "    './base_setting/babylonGPTKB_termination_seed1_EN_temp0_6th.db',\n",
    "    './base_setting/babylonGPTKB_termination_seed1_EN_temp0_7th.db',\n",
    "    './base_setting/babylonGPTKB_termination_seed1_EN_temp0_8th.db',\n",
    "    './base_setting/babylonGPTKB_termination_seed1_EN_temp0_9th.db',\n",
    "    './base_setting/babylonGPTKB_termination_seed1_EN_temp0_10th.db',\n",
    "\n",
    "    #'./seed_variation/babylonGPTKB_termination_seed2_EN_temp0.db',\n",
    "    #'./seed_variation/babylonGPTKB_termination_seed3_EN_temp0.db',\n",
    "    #'./seed_variation/babylonGPTKB_termination_seed4_EN_temp0.db',\n",
    "    #'./seed_variation/babylonGPTKB_termination_seed5_EN_temp0.db',\n",
    "    #'./seed_variation/babylonGPTKB_termination_seed6_EN_temp0.db',\n",
    "    #'./seed_variation/babylonGPTKB_termination_seed7_EN_temp0.db',\n",
    "    #'./seed_variation/babylonGPTKB_termination_seed8_EN_temp0.db',\n",
    "    #'./seed_variation/babylonGPTKB_termination_seed9_EN_temp0.db',\n",
    "    #'./seed_variation/babylonGPTKB_termination_seed10_EN_temp0.db',\n",
    "\n",
    "    #'./rand_variation/babylonGPTKB_termination_seed1_EN_temp1_1st.db',\n",
    "    #'./rand_variation/babylonGPTKB_termination_seed1_EN_temp1_2nd.db',\n",
    "    #'./rand_variation/babylonGPTKB_termination_seed1_EN_temp1_3rd.db',\n",
    "    #'./rand_variation/babylonGPTKB_termination_seed1_EN_temp1_4th.db',\n",
    "    #'./rand_variation/babylonGPTKB_termination_seed1_EN_temp1_5th.db',\n",
    "    #'./rand_variation/babylonGPTKB_termination_seed1_EN_temp1_6th.db',\n",
    "    #'./rand_variation/babylonGPTKB_termination_seed1_EN_temp1_7th.db',\n",
    "    #'./rand_variation/babylonGPTKB_termination_seed1_EN_temp1_8th.db',\n",
    "    #'./rand_variation/babylonGPTKB_termination_seed1_EN_temp1_9th.db',\n",
    "    #'./rand_variation/babylonGPTKB_termination_seed1_EN_temp1_10th.db',\n",
    "\n",
    "    #'./lang_variation/backtranslation_EN/babylonGPTKB_DE-EN_20250630_100431.db',\n",
    "    #'./lang_variation/backtranslation_EN/babylonGPTKB_ES-EN_20250616_091934.db',\n",
    "    #'./lang_variation/backtranslation_EN/babylonGPTKB_FR-EN_20250627_151851.db',\n",
    "    #'./lang_variation/backtranslation_EN/babylonGPTKB_IT-EN_20250628_132420.db',\n",
    "    #'./lang_variation/backtranslation_EN/babylonGPTKB_PO-EN_20250623_102904.db',\n",
    "    #'./lang_variation/backtranslation_EN/babylonGPTKB_PT-EN_20250623_104443.db',\n",
    "    #'./lang_variation/backtranslation_EN/babylonGPTKB_RU-EN_20250612_163509.db',\n",
    "    #'./lang_variation/backtranslation_EN/babylonGPTKB_SV-EN_20250703_095332.db',\n",
    "    #'./lang_variation/backtranslation_EN/babylonGPTKB_TR-EN_20250620_104047.db',\n",
    "\n",
    "    #'./lang_variation/ID_babylonGPTKB_seed1_DE_temp0.db',\n",
    "    #'./lang_variation/ID_babylonGPTKB_seed1_ES_temp0.db',\n",
    "    #'./lang_variation/ID_babylonGPTKB_seed1_FR_temp0.db',\n",
    "    #'./lang_variation/ID_babylonGPTKB_seed1_IT_temp0.db',\n",
    "    #'./lang_variation/ID_babylonGPTKB_seed1_PO_temp0.db',\n",
    "    #'./lang_variation/ID_babylonGPTKB_seed1_PT_temp0.db',\n",
    "    #'./lang_variation/ID_babylonGPTKB_seed1_RU_temp0.db',\n",
    "    #'./lang_variation/ID_babylonGPTKB_seed1_SV_temp0.db',\n",
    "    #'./lang_variation/ID_babylonGPTKB_seed1_TR_temp0.db',\n",
    "\n",
    "    #'./topic_variation/TBBT/TBBT_GPTKB_seed1_EN_temp0_1.db',\n",
    "    #'./topic_variation/TBBT/TBBT_GPTKB_seed1_EN_temp0_2.db',\n",
    "    #'./topic_variation/TBBT/TBBT_GPTKB_seed1_EN_temp0_3.db',\n",
    "    #'./topic_variation/TBBT/TBBT_GPTKB_seed1_EN_temp0_4.db',\n",
    "    #'./topic_variation/TBBT/TBBT_GPTKB_seed1_EN_temp0_5.db',\n",
    "    #'./topic_variation/TBBT/TBBT_GPTKB_seed1_EN_temp0_6.db',\n",
    "    #'./topic_variation/TBBT/TBBT_GPTKB_seed1_EN_temp0_7.db',\n",
    "    #'./topic_variation/TBBT/TBBT_GPTKB_seed1_EN_temp0_8.db',\n",
    "    #'./topic_variation/TBBT/TBBT_GPTKB_seed1_EN_temp0_9.db',\n",
    "    #'./topic_variation/TBBT/TBBT_GPTKB_seed1_EN_temp0_10.db',\n",
    "\n",
    "    #'./topic_variation/dax40/dax40GPTKB_seed1_EN_temp0_1.db',\n",
    "    #'./topic_variation/dax40/dax40GPTKB_seed1_EN_temp0_2.db',\n",
    "    #'./topic_variation/dax40/dax40GPTKB_seed1_EN_temp0_3.db',\n",
    "    #'./topic_variation/dax40/dax40GPTKB_seed1_EN_temp0_4.db',\n",
    "    #'./topic_variation/dax40/dax40GPTKB_seed1_EN_temp0_5.db',\n",
    "    #'./topic_variation/dax40/dax40GPTKB_seed1_EN_temp0_6.db',\n",
    "    #'./topic_variation/dax40/dax40GPTKB_seed1_EN_temp0_7.db',\n",
    "    #'./topic_variation/dax40/dax40GPTKB_seed1_EN_temp0_8.db',\n",
    "    #'./topic_variation/dax40/dax40GPTKB_seed1_EN_temp0_9.db',\n",
    "    #'./topic_variation/dax40/dax40GPTKB_seed1_EN_temp0_10.db',\n",
    "    ]\n",
    "\n",
    "sql_query = \"SELECT name FROM node WHERE type=\\\"instance\\\" ;\"\n",
    "#sql_query = \"SELECT name FROM node WHERE type=\\\"literal\\\" ;\"\n",
    "#sql_query = \"SELECT name FROM predicate ;\"\n",
    "#sql_query = \"SELECT DISTINCT object FROM triple WHERE predicate = \\\"instanceOf\\\" ;\"\n",
    "#sql_query = \"SELECT DISTINCT object FROM triple WHERE predicate IN (\\'instanceOf\\',\\'InstanzVon\\',\\'instanciaDe\\',\\'instanceDe\\',\\'istanzaDi\\',\\'instancja\\',\\'instÃ¢nciaDe\\',\\'Ð§Ð°ÑÑ‚Ð½Ñ‹Ð¹Ð¡Ð»ÑƒÑ‡Ð°Ð¹Ð”Ð»Ñ\\',\\'Ã¤rInstansAv\\',\\'nedir\\');\"\n",
    "#sql_query = \"SELECT id FROM triple ;\"\n",
    "#sql_query = \"SELECT subject, predicate, object FROM triple WHERE subject = 'Hammurabi';\"\n",
    "\n",
    "def remove_disambiguator(label):\n",
    "    # Remove trailing ' (number)' from label, e.g., 'father (1)' -> 'father'\n",
    "    return re.sub(r' \\(\\d+\\)$', '', label)\n",
    "\n",
    "def safe_query(db_file, query, retries=50, delay=2):\n",
    "    \"\"\"Attempts to execute a query on an SQLite database with retries. Removes disambiguators from results.\"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            conn = sqlite3.connect(f'file:{db_file}?mode=ro', uri=True)\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(query)\n",
    "            fetch = cursor.fetchall()\n",
    "            result = set(row[0] for row in fetch)\n",
    "            #result = set(row for row in fetch)\n",
    "            return result, len(fetch)\n",
    "            \n",
    "            # If computing metrics for backtranslations, comment the two lines above and use the\n",
    "            # two lines below. Remove disambiguators from each string before adding to set\n",
    "\n",
    "            #cleaned = set(remove_disambiguator(row[0]) for row in fetch)\n",
    "            #return cleaned, len(cleaned)\n",
    "        except sqlite3.DatabaseError as e:\n",
    "            #print(f\"[{db_file}] Attempt {attempt+1} failed: {e}\")\n",
    "            time.sleep(delay)\n",
    "        finally:\n",
    "            if 'conn' in locals():\n",
    "                conn.close()\n",
    "    print(f\"[{db_file}] Failed after {retries} attempts.\")\n",
    "    return set(), 0\n",
    "\n",
    "# Storage for results\n",
    "all_sets = []\n",
    "tot_count = []\n",
    "\n",
    "# Main loop\n",
    "for db_file in db_files:\n",
    "    if not os.path.exists(db_file):\n",
    "        print(f\"File not found: {db_file}\")\n",
    "        continue\n",
    "\n",
    "    result_set, count = safe_query(db_file, sql_query)\n",
    "    all_sets.append(result_set)\n",
    "    tot_count.append(count)\n",
    "    #print(f\"{count}\")\n",
    "\n",
    "# Stats\n",
    "for i in tot_count:\n",
    "    print(i)\n",
    "\n",
    "print('\\n')\n",
    "mean = np.mean(tot_count)\n",
    "std = np.std(tot_count)\n",
    "print(f\"{mean} Â± {std:.2f}\")\n",
    "\n",
    "# Unpack the sets (optional â€“ only if exactly 10 DBs were processed successfully)\n",
    "if len(all_sets) == 10:\n",
    "    (set_1, set_2, set_3, set_4, set_5,\n",
    "     set_6, set_7, set_8, set_9, set_10) = all_sets\n",
    "else:\n",
    "    print(\"Warning: Less than 10 sets collected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2758162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVERAGE JACCARD SIMILARITY\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "def jaccard_similarity(list_a, list_b):\n",
    "    set_a, set_b = set(list_a), set(list_b)\n",
    "    return len(set_a & set_b) / len(set_a | set_b)\n",
    "\n",
    "# List of all sets\n",
    "sets = [set_1, set_2, set_3, set_4, set_5, set_6, set_7, set_8, set_9, set_10]\n",
    "set_names = ['set_1', 'set_2', 'set_3', 'set_4', 'set_5', 'set_6', 'set_7', 'set_8', 'set_9', 'set_10']\n",
    "\n",
    "# Initialize sum and count for averaging\n",
    "total_similarity = 0\n",
    "pair_count = 0\n",
    "\n",
    "# Compute and print all pairwise Jaccard similarities\n",
    "for (i, set_a), (j, set_b) in combinations(enumerate(sets), 2):\n",
    "    sim = jaccard_similarity(set_a, set_b)\n",
    "    print(f\"Jaccard({set_names[i]} âˆ© {set_names[j]}) = {sim:.4f}\")\n",
    "    total_similarity += sim\n",
    "    pair_count += 1\n",
    "\n",
    "# Compute average similarity\n",
    "average_similarity = total_similarity / pair_count\n",
    "std_similarity = np.std([jaccard_similarity(set_a, set_b) for (i, set_a), (j, set_b) in combinations(enumerate(sets), 2)])\n",
    "print(f\"\\nAverage pairwise Jaccard similarity: {average_similarity:.4f} Â± {std_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd6805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVERAGE COSINE-BASED HAUSDORFF SIMILARITY\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "\n",
    "# Load the sentence embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')\n",
    "#model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2', device='cuda')\n",
    "\n",
    "def average_cosine_hausdorff_similarity(set1, set2, match_threshold=0.95):\n",
    "    # Encode the entity sets\n",
    "    embeddings1 = model.encode(list(set1), convert_to_numpy=True, batch_size=64)\n",
    "    embeddings2 = model.encode(list(set2), convert_to_numpy=True, batch_size=64)\n",
    "\n",
    "    # Compute cosine distance matrices\n",
    "    distances_1 = cosine_distances(embeddings1, embeddings2)\n",
    "    distances_2 = cosine_distances(embeddings2, embeddings1)\n",
    "\n",
    "    # Compute average minimum distance (Hausdorff style) in both directions\n",
    "    avg_distance1 = np.mean(np.min(distances_1, axis=1))\n",
    "    avg_distance2 = np.mean(np.min(distances_2, axis=1))\n",
    "    avg_distance = (avg_distance1 + avg_distance2) / 2\n",
    "    avg_similarity = 1 - avg_distance\n",
    "\n",
    "    # Convert distances to similarities\n",
    "    sim_matrix_1 = 1 - distances_1\n",
    "    sim_matrix_2 = 1 - distances_2\n",
    "\n",
    "    # Count matches above the similarity threshold\n",
    "    matches1 = np.sum(np.max(sim_matrix_1, axis=1) >= match_threshold)\n",
    "    matches2 = np.sum(np.max(sim_matrix_2, axis=1) >= match_threshold)\n",
    "\n",
    "    len1 = len(set1)\n",
    "    len2 = len(set2)\n",
    "\n",
    "    pct1 = 100 * matches1 / len1 if len1 > 0 else 0\n",
    "    pct2 = 100 * matches2 / len2 if len2 > 0 else 0\n",
    "\n",
    "    return avg_similarity, matches1, matches2, pct1, pct2, len1, len2\n",
    "\n",
    "sets = [set_1, set_2, set_3, set_4, set_5, set_6, set_7, set_8, set_9, set_10]\n",
    "set_names = ['set_1', 'set_2', 'set_3', 'set_4', 'set_5', 'set_6', 'set_7', 'set_8', 'set_9', 'set_10']\n",
    "\n",
    "similarities = []\n",
    "avg_match_percentages = []\n",
    "pair_count = 0\n",
    "\n",
    "for (i, set_a), (j, set_b) in combinations(enumerate(sets), 2):\n",
    "    similarity, matches_a, matches_b, pct_a, pct_b, len_a, len_b = average_cosine_hausdorff_similarity(set_a, set_b)\n",
    "\n",
    "    avg_match_pct = (pct_a + pct_b) / 2\n",
    "\n",
    "    print(f\"Avg Cosine-based Hausdorff Similarity ({set_names[i]} & {set_names[j]}): {similarity:.4f}\")\n",
    "    print(f\"  Matches from {set_names[i]} â†’ {set_names[j]}: {matches_a}/{len_a} ({pct_a:.1f}%)\")\n",
    "    print(f\"  Matches from {set_names[j]} â†’ {set_names[i]}: {matches_b}/{len_b} ({pct_b:.1f}%)\")\n",
    "    print(f\"  ðŸ” Avg Match Percentage (both directions): {avg_match_pct:.1f}%\\n\")\n",
    "\n",
    "    similarities.append(similarity)\n",
    "    avg_match_percentages.append(avg_match_pct)\n",
    "    pair_count += 1\n",
    "\n",
    "# Final summary statistics\n",
    "overall_avg_similarity = np.mean(similarities)\n",
    "overall_std_similarity = np.std(similarities, ddof=1) if pair_count > 1 else 0\n",
    "\n",
    "overall_avg_match = np.mean(avg_match_percentages)\n",
    "overall_std_match = np.std(avg_match_percentages, ddof=1) if pair_count > 1 else 0\n",
    "\n",
    "print(f\"\\nOverall Avg Cosine-based Hausdorff Similarity (all pairs): {overall_avg_similarity:.4f} ({overall_avg_match:.2f}%) Â± {overall_std_similarity:.4f} ({overall_std_match:.2f}%)\")\n",
    "#print(f\"Overall Avg Match Percentage (all pairs): {overall_avg_match:.2f}% Â± {overall_std_match:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
